\documentclass[12pt, titlepage]{article}

\usepackage{fullpage}
\usepackage[round]{natbib}
\usepackage{multirow}
\usepackage{booktabs}
\usepackage{tabularx}
\usepackage{graphicx}
\usepackage{float}
\usepackage{hyperref}
\hypersetup{
    colorlinks,
    citecolor=blue,
    filecolor=black,
    linkcolor=red,
    urlcolor=blue
}

\input{../../Comments}
\input{../../Common}

\newcounter{acnum}
\newcommand{\actheacnum}{AC\theacnum}
\newcommand{\acref}[1]{AC\ref{#1}}

\newcounter{ucnum}
\newcommand{\uctheucnum}{UC\theucnum}
\newcommand{\uref}[1]{UC\ref{#1}}

\newcounter{mnum}
\newcommand{\mthemnum}{M\themnum}
\newcommand{\mref}[1]{M\ref{#1}}

\begin{document}

\title{System Design for \progname{}} 
\author{\authname}
\date{\today}

\maketitle

\pagenumbering{roman}

\section{Revision History}

\begin{tabularx}{\textwidth}{p{3cm}p{2cm}X}
\toprule {\bf Date} & {\bf Version} & {\bf Notes}\\
\midrule
January 18, 2023 & 1.0 & Initial System Design Draft\\

\bottomrule
\end{tabularx}

\newpage

\section{Reference Material}

This section records information for easy reference.

\subsection{Abbreviations and Acronyms}

\renewcommand{\arraystretch}{1.2}
\begin{tabular}{l l} 
  \toprule		
  \textbf{symbol} & \textbf{description}\\
  \midrule 
  \progname & Explanation of program name\\
  \wss{...} & \wss{...}\\
  \bottomrule
\end{tabular}\\

\newpage

\tableofcontents

\newpage

\listoftables

\listoffigures

\newpage

\pagenumbering{arabic}

\section{Introduction}
The purpose of this document will give an overview of the system components for how the user interacts with the system, 
and the communication between the design of the hardware, software, and any electrical components.

\section{Purpose}
The purpose of our project is to create a device that will translate sign language gestures into their corresponding words 
or phrases. This will require the creation and development of a computer vision system alongside a machine learning model that 
will be used to recognize the hand motions, as well as a Raspberry Pi that will speak the word or phrase. The user will perform 
the sign language motion that will be captured by our computer vision system through a camera, and processed by our machine learning 
model and spoken through our Raspberry Pi.


\section{Scope}
OpenASL is primarily designed to assist the hearing impaired who use sign language to communicate. The following goals listed 
describe the key requirements for OpenASL to efficiently translate gestures and motions for any individual who does not know sign language 
to understand. More detailed explanations for each goal can be found in the SRS (REF SRS 2.2.1).


\wss{I made two tables, choose the one you like}
\begin{center}
\begin{tabular}{ | m{18em} | } 
  \hline
  Goals\\ 
  \hline
  Reliable and Accurate Translations\\ 
  \hline
  Real Time Translations\\ 
  \hline
  Ease of Use\\
  \hline
  Affordability\\
  \hline
  Customizable to User\\
  \hline
\end{tabular}
\end{center}

\begin{center}
\begin{tabular} {m{18em}}
  \toprule		
  \textbf{Goals}\\
  \midrule 
  Reliable and Accurate Translations\\
  \hline
  Real Time Translations\\ 
  \hline
  Ease of Use\\
  \hline
  Affordability\\
  \hline
  Customizable to User\\
  \bottomrule
\end{tabular}\\
\end{center}
\section{Project Overview}

\subsection{Normal Behaviour}
OpenASL acts as a medium for sign language to spoken language to help the hearing impaired communicate without the need of a human translator. 
Under normal operations, the user would perform ASL gestures in front of a camera that would detect motion, which would begin having the Raspberry Pi 
start classifying the movement of the user with its database and output the corresponding English word/phrase through speakers for the other person to 
understand. The user is also able to train the algorithm to learn any of the user’s subtle differences in gestures from the standard ASL language to 
improve the accuracy of classification for words/phrases.

\subsection{Undesired Event Handling}
In the event of an undesired error during the translation, the system should stop the translation being spoken through the speaker and display on the 
user interface that an error has occurred to let the user know that something has happened to the system. In the case that an error occurs during the 
process of the user training the model, such that the system is unable to classify the gesture, an error message should display on the interface to 
tell the user to retry. This would help prevent any incorrect data from being entered into the classification database for more accurate results.

\subsection{Component Diagram}

\subsection{Connection Between Requirements and Design} \label{SecConnection}
(REF SRS 6.1, 6.2, 8.1, 8.2, 8.3, 8.4 for description of ID).

\wss{Missing the first table because I cannot figure out how to do long table}

\renewcommand{\arraystretch}{1.2}
\noindent \begin{tabularx}{\textwidth}{p{0.2\linewidth}|p{0.72\linewidth}}
\toprule
\textbf{Module} & \textbf{Requirements+}\\
\midrule
Motion Tracking
& CFR1, CFR2, MLFR1, MLFR2, MLFR3, MLFR5, MLFR6, NFR2\\
\hline
Keypoint Classifier
& NFR1, NFR5\\
\hline
Machine Learning
& MLFR4, MLFR7\\
\bottomrule
\end{tabularx}

\section{System Variables}
\subsection{Monitored Variables}

\renewcommand{\arraystretch}{1.2}
\noindent \begin{tabularx}{\textwidth}{p{0.15\linewidth}|p{0.12\linewidth}|p{0.12\linewidth}|p{0.5\linewidth}}
\toprule
\textbf{Monitor Name} & \textbf{Monitor Type} & \textbf{Range} & \textbf{Description}\\
\midrule
mode & Integer & [0, 2] & Program operation mode (normal, keypoint training, point history/motion training)\\
\hline
num & Integer & [0,25] & Determines corresponding ASL classifier label to use\\
\hline
landmark\_list & List & [-1.0,1.0] & Set of normalized coordinates to be paired with ASL classifier label\\
\bottomrule
\end{tabularx}

\subsection{Controlled Variables}

\renewcommand{\arraystretch}{1.2}
\noindent \begin{tabularx}{\textwidth}{p{0.2\linewidth}|p{0.12\linewidth}|p{0.12\linewidth}|p{0.45\linewidth}}
\toprule
\textbf{Control Name} & \textbf{Control Type} & \textbf{Value} & \textbf{Description}\\
\midrule
RANDOM\_SEED & Integer & 51 & Value set to control random shuffling in ML model for reproducible output\\
\bottomrule
\end{tabularx}

\subsection{Constants Variables}

\renewcommand{\arraystretch}{1.2}
\noindent \begin{tabularx}{\textwidth}{p{0.28\linewidth}|p{0.12\linewidth}|p{0.08\linewidth}|p{0.4\linewidth}}
\toprule
\textbf{Constant Name} & \textbf{Constant Type} & \textbf{Value} & \textbf{Description}\\
\midrule
min\_detection\_confidence & Integer & 0.5 & Minimum confidence for hand detection from tracking script\\
\hline
num & Integer & 0.5 & Minimum confidence for hand joint tracking\\
\hline
landmark\_list & Integer & 16 & Maximum point history entries for motion tracking\\
\hline
NUM\_CLASSES & Integer & 26 & Number of classifier labels in ML model\\
\bottomrule
\end{tabularx}

\section{User Interfaces}

The user interface is designed to give the user the choice between translating ASL to spoken language and training the machine learning module to adapt 
to any of the user’s habits for any phrases. For translating ASL, the user would interact with the Raspberry Pi Camera that is equipped to register hand 
movement for the machine learning algorithm to classify. Either on a PC or laptop, the English translation will be provided to the user to determine if 
it is accurate or if it requires additional training. For training, the interface will display when to do the gesture in front of the camera to retrain 
the classification module for more accurate results. For the audience, who do not know sign language, a text-to-speech module will deliver a translation of ASL.

\section{Design of Hardware}

\wss{Most relevant for mechatronics projects}
\wss{Show what will be acquired}
\wss{Show what will be built, with detail on fabrication and materials}
\wss{Include appendices as appropriate, possibly with sketches, drawings, CAD, etc}

\section{Design of Electrical Components}

N/A

\section{Design of Communication Protocols}

N/A

\section{Timeline}

\wss{Schedule of tasks and who is responsible}

\renewcommand{\arraystretch}{1.2}
\noindent \begin{tabularx}{\textwidth}{p{0.3\linewidth}|p{0.3\linewidth}|p{0.35\linewidth}}
\toprule
\textbf{Objective} & \textbf{Date to be completed by} & \textbf{Member(s) Responsible}\\
\midrule
Create an interface that lets the user switch between training and translating & Jan 20, 2023 & Runze Zhu and Jiahui Chen\\
\hline
Add an expanded vocabulary of common phrases into the database & Jan 31, 2023 & Robert Zhu\\
\hline
Build and connect Raspberry Pi to OpenCV system to provide real-time translation & Feb 5, 2023 & Zifan Meng\\
\hline
Program a text-to-speech algorithm & Feb 5, 2023 & Nafi Hasan\\
\hline
Move OpenCV system and machine learning model onto Raspberry Pi & Feb 10, 2023 & Kelvin Huynh\\
\bottomrule
\end{tabularx}

% \bibliographystyle {plainnat}
% \bibliography{../../../refs/References}

\newpage{}

\appendix

%\section{Interface}

%\section{Mechanical Hardware}

%\section{Electrical Components}

%\section{Communication Protocols}

\section{Reflection}

The information in this section will be used to evaluate the team members on the
graduate attribute of Problem Analysis and Design.  Please answer the following questions:

\begin{enumerate}
  \item What are the limitations of your solution?  Put another way, given
  unlimited resources, what could you do to make the project better? (LO\_ProbSolutions)
  ~\\
  \\
  Robert Zhu: One of the limitations for our solution is that it is unable to capture the full language of ASL through only capturing hand gestures. 
  That is because ASL often uses a range of different body movements to deliver a proper sentence. For example, the phrase for “come here” involves 
  tapping the knee, which our program is unable to categorize since it only tracks hand movement. Grammar is also an issue as face expressions dictate 
  the tone, urgency, and even the meaning of phrases when combined with hand gestures. At the moment, these aspects of ASL are out of the scope for the 
  current plan, however, with enough time and datasets from the ASL community that the machine learning algorithm can read from, more of this language 
  can be translated.\\
  \\
  Zifan Meng: One of the limitations of our solution is we cannot provide customization to each individual user. Every user has their own habits about hand gestures, for our current solution, we only have a universal training model for standard ASL gestures, if a user’s hand gesture differs a lot from the standard ASL gesture, it is likely that the translation is incorrect. If we were to have more development time, we could develop a user accounts function, each user has their own account and their own database, some specific gestures of theirs can be stored in the database and the product is able to translate accordingly to the account that’s logged in.\\

  
  \item Give a brief overview of other design solutions you considered.  What
  are the benefits and tradeoffs of those other designs compared with the chosen
  design?  From all the potential options, why did you select documented design?
  (LO\_Explores)
  ~\\
  \\
  Robert Zhu: One other design solution involved designing a device that is placed on the hands of the user with sensors that are capable of capturing 
  hand gestures, and transmitting the information into a spoken language. The main benefit of this method compared to our current chosen design would 
  be a very high accuracy in being able to classify the motion. Having sensors on each joint would provide more information for the processing unit by 
  being able to distinctly tell the position of each finger, leading to fewer mistakes compared to using a camera sensor. We did not select this method 
  as it greatly reduced the scope of ASL to only having finger movements. ASL uses dynamic motion that can not be captured using glove sensors, while a 
  camera sensor is able to detect these movements and classify them. A dataset is also easier to gather through using a camera sensor as the hardware 
  required for gloves requires a lot of effort to generate enough datasets to accurately translate.\\
  \\
  Zifan Meng: One design solution we considered was implementing a LED screen on Raspberry Pi that outputs the translation results. Our current solution is to have a speaker for audio output, if we have both visual output and audio output, the device is more complete as a portable, individually-working device, and is suitable for all users (users with other disabilities). However this solution significantly increases the complexity of the design and increases the cost of the device. The design that we have right now are targeting users that do not have hearing disabilities, hence the audio output is sufficient enough for helping most of the users to understand ASL languages.\\
\end{enumerate}

\end{document}