\documentclass[12pt, titlepage]{article}

\usepackage{amsmath, mathtools}

\usepackage[round]{natbib}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{graphicx}
\usepackage{colortbl}
\usepackage{xr}
\usepackage{hyperref}
\usepackage{longtable}
\usepackage{xfrac}
\usepackage{tabularx}
\usepackage{float}
\usepackage{siunitx}
\usepackage{booktabs}
\usepackage{multirow}
\usepackage[section]{placeins}
\usepackage{caption}
\usepackage{fullpage}

\hypersetup{
bookmarks=true,     % show bookmarks bar?
colorlinks=true,       % false: boxed links; true: colored links
linkcolor=red,          % color of internal links (change box color with linkbordercolor)
citecolor=blue,      % color of links to bibliography
filecolor=magenta,  % color of file links
urlcolor=cyan          % color of external links
}

\usepackage{array}

\externaldocument{../../SRS/SRS}

\input{../../Comments}
\input{../../Common}

\begin{document}

\title{Module Interface Specification for \progname{}} 
\author{\authname}
\date{\today}

\maketitle

\pagenumbering{roman}

\section{Revision History}

\begin{tabularx}{\textwidth}{p{3cm}p{2cm}X}
\toprule {\bf Date} & {\bf Version} & {\bf Notes}\\
\midrule
January 18, 2023 & 1.0 & Everyone - Initial MIS Draft\\
April 05, 2023 & 2.0 & Everyone - Revised MIS\\
\bottomrule
\end{tabularx}

~\newpage

\section{Symbols, Abbreviations and Acronyms}

See SRS Documentation at \url{https://github.com/kelhuynh/OpenASL/blob/main/docs/SRS/SRS.pdf}

\newpage

\tableofcontents

\newpage

\pagenumbering{arabic}

\section{Introduction}

The following document details the Module Interface Specifications for
OpenASL, a device developed with the aim of translating sign language into text-to-speech, 
with the purpose of helping members of the deaf and mute community communicate with those 
who do not know sign language.\\

\noindent Complementary documents include the System Requirement Specifications
and Module Guide.  The full documentation and implementation can be
found at \url{https://github.com/kelhuynh/OpenASL/}.\\

\section{Module Decomposition}

The following table is taken directly from the Module Guide document for this project.

\begin{table}[h!]
\centering
\begin{tabular}{p{0.3\textwidth} p{0.6\textwidth}}
\toprule
\textbf{Level 1} & \textbf{Level 2}\\
\midrule
\multirow{2}{0.3\textwidth}{Hardware-Hiding Module} & Video Capture Module \\
& Hardware Hiding Module\\
\midrule
\multirow{5}{0.3\textwidth}{Behaviour-Hiding Module} & User Interface Module\\
& Data Processing Module - Communicates with ML module with data from coordinate normalization module\\
& Data Collection Module - Communicates with ML module to update dataset\\
& Coordinate Export Module - Read data from video capture and stores into file\\
& Gesture Detection Module - Controller (ties everything together)\\
& Dynamic Gesture Detection Module\\

\midrule

\multirow{3}{0.3\textwidth}{Software Decision Module} & Video Analysis Module - requires data to be used\\
& Machine Learning Module\\
& Dynamic Machine Learning Module\\
& Coordinate Normalization Module\\
\bottomrule


\end{tabular}
\caption{Module Hierarchy}
\label{TblMH}
\end{table}


\newpage

\section{MIS of Gesture Detection Module (M1)} \label{m1}

\subsection{Module}

motionTrack

\subsection{Uses}

Video Capture, Coordinate Normalization, Coordinate Export, Video Analysis, Keypoint Classification, Text to Speech

\subsection{Syntax}

\subsubsection{Exported Constants}

\begin{center}
\begin{tabular}{p{5cm} p{3cm} p{3cm} p{4cm}}
\hline
\textbf{Name} & \textbf{In} & \textbf{Out} & \textbf{Exceptions} \\
\hline
results & image & Object & - \\
hand\_landmarks & - & Tuple of tuples & - \\
handedness & - & \mathbb{R} & - \\
\hline
\end{tabular}
\end{center}

\subsubsection{Exported Access Programs}

\begin{center}
\begin{tabular}{p{5cm} p{3cm} p{3cm} p{4cm}}
\hline
\textbf{Name} & \textbf{In} & \textbf{Out} & \textbf{Exceptions} \\
\hline
motionTrack & - & - & cv2.error\\
\hline
\end{tabular}
\end{center}

\subsection{Semantics}

\subsubsection{State Variables}

None

\subsubsection{Environment Variables}

f - file variable for coordinate export purposes

\subsubsection{Assumptions}

None

\subsubsection{Access Routine Semantics}

\noindent motionTrack():
\begin{itemize}
\item output: video consisting of overlay for hand gesture classification into ASL\\
\item exception: exc := cv2.error
\end{itemize}

~\newpage

\section{MIS of Coordinate Normalization Module (M2)} \label{m2}

\subsection{Module}

Coordinate Normalization

\subsection{Uses}

Video Capture

\subsection{Syntax}

\subsubsection{Exported Constants}

\begin{center}
\begin{tabular}{p{5cm} p{3cm} p{3cm} p{4cm}}
\hline
\textbf{Name} & \textbf{In} & \textbf{Out} & \textbf{Exceptions} \\
\hline
pre\_processed\_landmark\_list & landmark\_list & Tuple of tuples & - \\
\hline
\end{tabular}
\end{center}

\subsection{Semantics}

\subsubsection{State Variables}

None

\subsubsection{Environment Variables}

None

\subsubsection{Assumptions}

None

\subsubsection{Access Routine Semantics}

\noindent pre\_process\_landmark(landmark\_list):
\begin{itemize}
\item output: tuple of 20 tuples consisting of x and y coordinates for each hand joint
\item exception: exc := ListIndexOutofBounds
\end{itemize}

\subsubsection{Local Functions}
\begin{itemize}
\item \_\_calc\_landmark\_list
\end{itemize}

~\newpage

\section{MIS of Coordinate Export Module (M3)} \label{m3}

\subsection{Module}

Coordinate Export

\subsection{Uses}

Coordinate Normalization

\subsection{Syntax}

\subsubsection{Exported Constants}

\begin{center}
\begin{tabular}{p{5cm} p{3cm} p{3cm} p{4cm}}
\hline
\textbf{Name} & \textbf{In} & \textbf{Out} & \textbf{Exceptions} \\
\hline
keypoint.csv & - & File containing normalized coordinates & - \\
\hline
\end{tabular}
\end{center}

\subsection{Semantics}

\subsubsection{State Variables}

None

\subsubsection{Environment Variables}

None

\subsubsection{Assumptions}

None

\subsubsection{Local Functions}
\begin{itemize}
\item \_\_make\_csv
\end{itemize}

~\newpage

\section{MIS of Video Capture Module (M4)} \label{m4}

\subsection{Module}

Video Capture

\subsection{Uses}

None

\subsection{Syntax}

\subsubsection{Exported Constants}

\begin{center}
\begin{tabular}{p{5cm} p{3cm} p{3cm} p{4cm}}
\hline
\textbf{Name} & \textbf{In} & \textbf{Out} & \textbf{Exceptions} \\
\hline
success & - & \mathbb{R} & - \\
image & - & Object & - \\
\hline
\end{tabular}
\end{center}

\subsection{Semantics}

\subsubsection{State Variables}

None

\subsubsection{Environment Variables}

success - indicates that the camera input is initialized for use

\subsubsection{Assumptions}

There is an available camera connected to the system

~\newpage

\section{MIS of Video Analysis Module (M5)} \label{m5}

\subsection{Module}

Video Analysis

\subsection{Uses}

Coordinate Normalization, Video Capture

\subsection{Syntax}

\subsubsection{Exported Access Programs}

\begin{center}
\begin{tabular}{p{5cm} p{3cm} p{3cm} p{4cm}}
\hline
\textbf{Name} & \textbf{In} & \textbf{Out} & \textbf{Exceptions} \\
\hline
draw\_bounding\_rect & - & image & - \\
draw\_landmarks & - & image & - \\
draw\_info\_text & - & image & - \\
\hline
\end{tabular}
\end{center}

\subsection{Semantics}

\subsubsection{State Variables}

None

\subsubsection{Environment Variables}

None

\subsubsection{Assumptions}

None

\subsubsection{Access Routine Semantics}

\noindent draw\_bounding\_rect(self, use\_brect, image, brect):
\begin{itemize}
\item output: image with overlaid bounding rectangle around hand
\item exception: exc := None
\end{itemize}

\noindent draw\_landmarks(self, image, landmark\_point):
\begin{itemize}
\item output: image with overlaid hand joints and connections
\item exception: exc := None
\end{itemize}

\noindent draw\_info\_text(self, image, brect, handedness, hand\_sign\_text):
\begin{itemize}
\item output:  image with overlaid classifier label
\item exception: exc := None
\end{itemize}

\subsubsection{Local Functions}
\begin{itemize}
\item \_\_calc\_bounding\_rect
\end{itemize}

~\newpage
\section{MIS of Dynamic Gesture Detection Module (M6)} \label{M10}

\subsection{Module}

Gesture Detection\\

\subsection{Uses}

Video Capture, Coordinate Normalization, Video Analysis\\

\subsection{Syntax}

\subsubsection{Exported Constants}
-

\subsubsection{Exported Access Programs}
\begin{center}
\begin{tabular}{p{5cm} p{3cm} p{3cm} p{4cm}}
\hline
\textbf{Name} & \textbf{In} & \textbf{Out} & \textbf{Exceptions} \\
\hline
mediapipe_dection & 
Image: frame from CV2

Model: Mediapipe model that processes the image frame
 & Image: Processed Image frame

Results: Processed Data from Mediapipe model
 & cv2.error \\
\hline
\end{tabular}
\end{center}

\subsection{Semantics}

\subsubsection{State Variables}

Image: refers to a single frame of a video, which is read from a cv2 object\\

\subsubsection{Environment Variables}

Cap: an instance of the cv2.VideoCapture class, which is used to capture video from a camera device connected to the system\\

\subsubsection{Assumptions}

None\\

\subsubsection{Access Routine Semantics}

mediapipe\_detection(): takes image frame and model as input and processes the image frame through the model, and returns the processed image frame and results to make detections in the video feed\\
\\
sel\_mode(): takes a key and mode as input to change between different modes of operation\\
\\
cap.read() used to read frames from the video capture device\\

~\newpage

\section{MIS of Data Processing Module (M7)} \label{M6}

\subsection{Module}

Keypoint Classification\\

\subsection{Uses}

Coordinate Export, Machine Learning\\

\subsection{Syntax}

\subsubsection{Exported Constants}

\begin{center}
\begin{tabular}{p{5cm} p{3cm} p{3cm} p{4cm}}
\hline
\textbf{Name} & \textbf{In} & \textbf{Out} & \textbf{Exceptions} \\
\hline
result\_index & landmark_list & R & ListIndexOutofRange \\
\hline
\end{tabular}
\end{center}

\subsection{Semantics}

\subsubsection{State Variables}

None\\

\subsubsection{Environment Variables}

None\\

\subsubsection{Assumptions}

None\\

\subsubsection{Local Functions}
\begin{itemize}
\item \_\_call\_\_
\end{itemize}

~\newpage

\section{MIS of Data Collection Module (M8)} \label{M9}

\subsection{Module}

DataCollection\\

\subsection{Uses}

Coordinate Normalization, Coordinate Export\\

\subsection{Syntax}

\subsubsection{Exported Constants}

\begin{center}
\begin{tabular}{p{5cm} p{3cm} p{3cm} p{4cm}}
\hline
\textbf{Name} & \textbf{In} & \textbf{Out} & \textbf{Exceptions} \\
\hline
actions & Number of Gestures & - & - \\
\hline
\end{tabular}
\end{center}

\subsubsection{Exported Access Programs}
-

\subsection{Semantics}

\subsubsection{State Variables}

No\_sequences: Represents the number of sequences to be recorded for each action

\subsubsection{Environment Variables}

Dirmax: Stores the highest amount of files within a gesture\\
\\
DATA\_PATH:  a list of directories where executable files are located\\

\subsubsection{Assumptions}

None\\

\subsubsection{Access Routine Semantics}

UserInput(): Takes user input and returns a string “action” and checks if action is present in the action list. If it is, it finds the max value of the directories present in the path and creates a directory with a name greater than the max value\\

~\newpage

\section{MIS of Machine Learning Module (M9)} \label{M7}

\subsection{Module}

ML Train\\

\subsection{Uses}

Coordinate Export\\

\subsection{Syntax}

\subsubsection{Exported Constants}

\begin{center}
\begin{tabular}{p{5cm} p{3cm} p{3cm} p{4cm}}
\hline
\textbf{Name} & \textbf{In} & \textbf{Out} & \textbf{Exceptions} \\
\hline
keypoint\_classifier.hdf5 & - & Hierarchical Data Format file & - \\
keypoint\_classifier.tflite & - & ML model & - \\
\hline
\end{tabular}
\end{center}

\subsection{Semantics}

\subsubsection{State Variables}

None\\

\subsubsection{Environment Variables}

None\\

\subsubsection{Assumptions}

None\\

\newpage
\section{MIS of Dynamic Machine Learning Module (M10)} \label{M11}

\subsection{Module}

machineLearning\\

\subsection{Uses}

Keypoint Classification, Coordinate Export, Coordinate Normalization, Training\\

\subsection{Syntax}

\subsubsection{Exported Constants}
-

\subsubsection{Exported Access Programs}
-

\subsection{Semantics}

\subsubsection{State Variables}

X\_train: Numpy array containing the training data for the machine learning model.\\
\\
X\_test: Numpy array containing the testing data for the machine learning model.\\
\\
y\_train: Numpy array containing the labels for the training data.\\
\\
y\_test: Numpy array containing the labels for the testing data.\\
\\
model: Keras sequential model object representing the machine learning model. Its weights are updated during training.\\

\subsubsection{Environment Variables}

None\\

\subsubsection{Assumptions}

None\\

\subsubsection{Access Routine Semantics}

np.loadtxt(): loads gesture labels into an array.\\
\\
os.listdir(): lists the contents of a directory.\\
\\
np.load(): loads a NumPy array from a binary file.\\
\\
train\_test\_split(): splits the dataset into training and testing sets.\\
\\
Sequential(): initializes the machine learning model.\\
\\
model.add(): adds layers to the machine learning model.\\
\\
model.compile(): configures the machine learning model for training.\\
\\
model.fit(): trains the machine learning model on the dataset.\\
\\
model.predict(): uses the trained model to make predictions on the test set.\\
\\
np.argmax(): gets the indices of the maximum values along an axis of a NumPy array.\\
\\
multilabel\_confusion\_matrix(): computes a confusion matrix for multiclass classification.\\
\\
model.save(): saves the trained machine learning model to a file.\\

\newpage

\bibliographystyle {plainnat}
\bibliography {../../../refs/References}

\end{document}